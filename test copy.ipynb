{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "from tools import logging, import_datasets\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: f'{x :.2f}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "from get_dataframes import GetDataframes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tools import (\n",
    "    import_config,\n",
    "    import_datasets,\n",
    "    if_tt_remove,\n",
    "    replace_ids_with_titles,\n",
    ")\n",
    "\n",
    "from plotting import (\n",
    "    actors_top_1_by_decades,\n",
    "    actors_top_10_by_genres,\n",
    "    actors_top_by_movies,\n",
    "    actors_top_10_by_votes,\n",
    ")\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from polars_tools import (\n",
    "    order_and_rename_pl,\n",
    "    apply_decade_column_pl,\n",
    "    col_to_keep_pl,\n",
    "    col_renaming_pl,\n",
    "    decode_clean_pl,\n",
    "    safe_literal_eval_pl,\n",
    "    import_datasets_pl,\n",
    ")\n",
    "from tools import (\n",
    "    get_tsv_files,\n",
    ")\n",
    "from cleaner import DataCleaner\n",
    "clean = DataCleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movie Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_files = \"movies_datasets\"\n",
    "default_path = \"clean_datasets\"\n",
    "fix_n = \"\\\\N\"\n",
    "\n",
    "first_df = pl.read_csv(\n",
    "    get_tsv_files(tsv_files)[\"title_basics\"],\n",
    "    separator='\\t',\n",
    "    null_values=fix_n,\n",
    "    ignore_errors=True\n",
    ")\n",
    "movies = first_df.filter(\n",
    "    (pl.col(\"titleType\") == \"movie\") &\n",
    "    (pl.col(\"isAdult\") == 0)\n",
    ")\n",
    "title_ratings = pl.read_csv(\n",
    "    get_tsv_files(tsv_files)[\"title_ratings\"],\n",
    "    separator='\\t',\n",
    "    null_values=fix_n,\n",
    "    ignore_errors=True\n",
    ")\n",
    "mov_rating = movies.join(\n",
    "    title_ratings,\n",
    "    left_on=\"tconst\",\n",
    "    right_on=\"tconst\",\n",
    ")\n",
    "\n",
    "df_imdb = pl.read_parquet(get_tsv_files(default_path)[\"imdb_full\"])\n",
    "\n",
    "merged = mov_rating.join(\n",
    "    df_imdb,\n",
    "    left_on=\"tconst\",\n",
    "    right_on=\"imdb_id\",\n",
    ")\n",
    "\n",
    "merged = merged.drop(clean.columns_to_drop_tmdb())\n",
    "merged = merged.rename({\"genres_right\": \"genres\"})\n",
    "\n",
    "akas = pl.read_csv(\n",
    "    get_tsv_files(tsv_files)[\"title_akas\"],\n",
    "    separator='\\t',\n",
    "    null_values=fix_n,\n",
    "    ignore_errors=True\n",
    ")\n",
    "region_only = akas.select([\"titleId\", \"region\"])\n",
    "fr_only = region_only.filter(pl.col(\"region\") == \"FR\")\n",
    "df = merged.join(\n",
    "    fr_only,\n",
    "    left_on=\"tconst\",\n",
    "    right_on=\"titleId\",\n",
    ")\n",
    "filtered = df.filter(pl.col(\"status\") == \"Released\")\n",
    "filtered = apply_decade_column_pl(filtered)\n",
    "filtered = order_and_rename_pl(filtered, col_to_keep_pl(\"movies\"), col_renaming_pl(\"movies\"))\n",
    "drop_nan = filtered.drop_nulls()\n",
    "drop_dup = drop_nan.filter(~pl.col(\"titre_id\").is_duplicated())\n",
    "print(f'drop_nan = {len(filtered) - len(drop_nan)}')\n",
    "print(f'drop_dup = {len(drop_nan) - len(drop_dup)}')\n",
    "drop_dup.write_parquet(\"testing.parquet\")\n",
    "\n",
    "\n",
    "tp = pd.read_parquet(\"clean_datasets/tmdb_full.parquet\")\n",
    "tp = tp[tp[\"release_date\"] != None]\n",
    "tp.sort_values(by=\"release_date\")\n",
    "tp = tp[~tp.release_date.isna()]\n",
    "tp.sort_values(by=\"release_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataframes_full_polars import GetDataframes\n",
    "# from get_dataframes import GetDataframes\n",
    "config = import_config()\n",
    "test = GetDataframes(config)\n",
    "\n",
    "# pl.read_parquet(\"clean_datasets/persons.parquet\")\n",
    "test.get_movies_dataframe() # shape: (13_012_610, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Person Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_files = \"movies_datasets\"\n",
    "default_path = \"clean_datasets\"\n",
    "fix_n = \"\\\\N\"\n",
    "logging.info(\"Import Name Basics !\")\n",
    "first_df = pl.read_csv(\n",
    "    get_tsv_files(tsv_files)[\"name_basics\"],\n",
    "    separator='\\t',\n",
    "    null_values=fix_n,\n",
    "    ignore_errors=True\n",
    ")\n",
    "first_df = first_df.drop([\"deathYear\", \"primaryProfession\"])\n",
    "logging.info(\"Spliting and modifing dtypes...\")\n",
    "first_df = first_df.fill_null(\"Unknow\")\n",
    "df = first_df.with_columns(\n",
    "    pl.when(pl.col('knownForTitles').is_not_null())\n",
    "    .then(pl.col('knownForTitles').str.split(','))\n",
    "    .alias('knownForTitles')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataframes_full_polars import GetDataframes\n",
    "# from get_dataframes import GetDataframes\n",
    "config = import_config()\n",
    "test = GetDataframes(config)\n",
    "\n",
    "pl.read_parquet(\"clean_datasets/persons.parquet\").head()\n",
    "# test.get_persons_dataframes() # shape: (13_012_610, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Characters Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_files = \"movies_datasets\"\n",
    "default_path = \"clean_datasets\"\n",
    "fix_n = \"\\\\N\"\n",
    "logging.info(\"Import Characters !\")\n",
    "first_df = pl.read_csv(\n",
    "    get_tsv_files(tsv_files)[\"title_principals\"],\n",
    "    separator='\\t',\n",
    "    null_values=fix_n,\n",
    "    ignore_errors=True\n",
    ")\n",
    "first_df = first_df.drop([\"job\", \"characters\", \"ordering\"])\n",
    "logging.info(\"Spliting and modifing dtypes...\")\n",
    "first_df = first_df.fill_null(\"Unknow\")\n",
    "df = first_df.with_columns(\n",
    "    pl.when(pl.col('category').is_not_null())\n",
    "    .then(pl.col('category').str.split(','))\n",
    "    .alias('category')\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataframes_full_polars import GetDataframes\n",
    "# from get_dataframes import GetDataframes\n",
    "config = import_config()\n",
    "test = GetDataframes(config)\n",
    "\n",
    "# pl.read_parquet(\"clean_datasets/characters_test.parquet\")\n",
    "test.get_characters_dataframe().head() # shape: (13_012_610, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actors Dataframe**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"clean_datasets/characters_test.parquet\")\n",
    "logging.info(f\"Get Actors only...\")\n",
    "actors_list = [\"self\", \"actor\", \"actress\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.explode(\"category\")\n",
    "    .filter(pl.col(\"category\").is_in(actors_list))\n",
    "    .with_columns(pl.col(\"category\").str.split(\",\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataframes_full_polars import GetDataframes\n",
    "# from get_dataframes import GetDataframes\n",
    "config = import_config()\n",
    "test = GetDataframes(config)\n",
    "\n",
    "# pl.read_parquet(\"clean_datasets/characters_test.parquet\")\n",
    "test.get_actors_dataframe().head() # shape: (13_012_610, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directors Dataframe**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"clean_datasets/characters_test.parquet\")\n",
    "logging.info(f\"Get Directors only...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.explode(\"category\")\n",
    "    .filter(pl.col(\"category\").str.contains(\"director\"))\n",
    "    .with_columns(pl.col(\"category\").str.split(\",\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataframes_full_polars import GetDataframes\n",
    "# from get_dataframes import GetDataframes\n",
    "config = import_config()\n",
    "test = GetDataframes(config)\n",
    "\n",
    "# pl.read_parquet(\"clean_datasets/characters_test.parquet\")\n",
    "df = test.get_directors_dataframe() # shape: (13_012_610, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actors Movies Dataframe**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 10:03:26 INFO     Parquet loaded ! Importing actors_movies...\n"
     ]
    }
   ],
   "source": [
    "movies_actors_og = import_datasets_pl(\"clean_datasets/actors_movies.parquet\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 10:21:28 INFO     Parquet loaded ! Importing actors_test...\n",
      "2023-11-18 10:21:31 INFO     Parquet loaded ! Importing persons_test...\n",
      "2023-11-18 10:21:32 INFO     Parquet loaded ! Importing movies_test...\n",
      "2023-11-18 10:21:50 INFO     Order and Rename...\n",
      "2023-11-18 10:21:50 INFO     Replace tt by movies titles...\n"
     ]
    }
   ],
   "source": [
    "actors = import_datasets_pl(\"clean_datasets/actors_test.parquet\", \"parquet\")\n",
    "persons = import_datasets_pl(\"clean_datasets/persons_test.parquet\", \"parquet\")\n",
    "movies = import_datasets_pl(\"clean_datasets/movies_test.parquet\", \"parquet\")\n",
    "\n",
    "actors_names = actors.join(persons, on=\"nconst\")\n",
    "\n",
    "movies_actors = movies.join(\n",
    "    actors_names, left_on=\"titre_id\", right_on=\"tconst\"\n",
    ")\n",
    "\n",
    "movies_actors = order_and_rename_pl(\n",
    "    movies_actors,\n",
    "    col_to_keep_pl(\"actors_movies\"),\n",
    "    col_renaming_pl(\"actors_movies\")\n",
    ")\n",
    "\n",
    "\n",
    "movies_actors = movies_actors.to_pandas()\n",
    "# movies_actors = movies_actors[col_renaming(name)]\n",
    "logging.info(\"Replace tt by movies titles...\")\n",
    "dict_titre = (\n",
    "    movies_actors[[\"titre_id\", \"titre_str\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"titre_id\")\n",
    "    .to_dict()[\"titre_str\"]\n",
    ")\n",
    "movies_actors[\"person_film\"] = movies_actors[\n",
    "    \"person_film\"\n",
    "].apply(\n",
    "    lambda x: if_tt_remove(\n",
    "        replace_ids_with_titles(x, dict_titre)\n",
    "    )\n",
    ")\n",
    "df = pl.from_pandas(movies_actors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 10:54:18 INFO     \u001b[38;5;2m---------- Start creating movies ----------\u001b[0m\n",
      "2023-11-18 10:54:18 INFO     TSV files already exist.\n",
      "2023-11-18 10:54:18 INFO     Polars loaded ! Importing title_basics...\n",
      "2023-11-18 10:54:19 INFO     Filter Porn Movies...!\n",
      "2023-11-18 10:54:19 INFO     Polars loaded ! Importing title_ratings...\n",
      "2023-11-18 10:54:19 INFO     Join !\n",
      "2023-11-18 10:54:19 INFO     Parquet loaded ! Importing tmdb_full...\n",
      "2023-11-18 10:54:20 INFO     Polars loaded ! Importing title_akas...\n",
      "2023-11-18 10:54:23 INFO     Join !\n",
      "2023-11-18 10:54:23 INFO     Apply Decade !\n",
      "2023-11-18 10:54:23 INFO     Order and Rename...\n",
      "2023-11-18 10:54:23 INFO     drop_nan = 311\n",
      "2023-11-18 10:54:23 INFO     drop_dup = 12846\n",
      "2023-11-18 10:54:23 INFO     Dataframe movies_test ready to use!\n",
      "2023-11-18 10:54:24 INFO     \u001b[38;5;2m---------- Job Done for movies ! ----------\n",
      "\u001b[0m\n",
      "2023-11-18 10:54:24 INFO     \u001b[38;5;2m---------- Start creating movies_cleaned ----------\u001b[0m\n",
      "2023-11-18 10:54:24 INFO     TSV files already exist.\n",
      "2023-11-18 10:54:24 INFO     Parquet loaded ! Importing movies_test...\n",
      "2023-11-18 10:54:24 INFO     Dataframe movies_cleaned_test ready to use!\n",
      "2023-11-18 10:54:24 INFO     \u001b[38;5;2m---------- Job Done for movies_cleaned ! ----------\n",
      "\u001b[0m\n",
      "2023-11-18 10:54:24 INFO     \u001b[38;5;2m---------- Start creating actors_movies ----------\u001b[0m\n",
      "2023-11-18 10:54:24 INFO     TSV files already exist.\n",
      "2023-11-18 10:54:24 INFO     File actors_test.parquet not found. Creation...\n",
      "2023-11-18 10:54:24 INFO     File characters_test.parquet not found. Creation...\n",
      "2023-11-18 10:54:24 INFO     Polars loaded ! Importing title_principals...\n",
      "2023-11-18 10:54:27 INFO     Spliting and modifing dtypes...\n",
      "2023-11-18 10:54:30 INFO     Writing characters_test dataframe...\n",
      "2023-11-18 10:55:16 INFO     Dataframe characters_test ready to use!\n",
      "2023-11-18 10:55:16 INFO     Get actors_test only...\n",
      "2023-11-18 10:55:32 INFO     Writing actors_test dataframe...\n",
      "2023-11-18 10:55:47 INFO     Dataframe actors_test ready to use!\n",
      "2023-11-18 10:55:47 INFO     File persons_test.parquet not found. Creation...\n",
      "2023-11-18 10:55:47 INFO     Polars loaded ! Importing name_basics...\n",
      "2023-11-18 10:55:49 INFO     Spliting and modifing dtypes...\n",
      "2023-11-18 10:55:50 INFO     Writing persons_test dataframe...\n",
      "2023-11-18 10:55:58 INFO     Dataframe persons_test ready to use!\n",
      "2023-11-18 10:55:58 INFO     Parquet loaded ! Importing movies_cleaned_test...\n",
      "2023-11-18 10:55:58 INFO     Values modified ? False\n",
      "2023-11-18 10:55:58 INFO     No need to update movies, all values are equals.\n",
      "2023-11-18 10:55:58 INFO     Dataframe movies_cleaned_test ready to use!\n",
      "2023-11-18 10:55:58 INFO     Parquet loaded ! Importing movies_cleaned_test...\n",
      "2023-11-18 10:55:58 INFO     Join actors with persons!\n",
      "2023-11-18 10:56:17 INFO     Join actors with movies!\n",
      "2023-11-18 10:56:17 INFO     Order and Rename...\n",
      "2023-11-18 10:56:17 INFO     Replace tt by movies titles...\n",
      "2023-11-18 10:56:17 INFO     Writing actors_movies_test dataframe...\n",
      "2023-11-18 10:56:17 INFO     Dataframe actors_movies_test ready to use!\n",
      "2023-11-18 10:56:17 INFO     \u001b[38;5;2m---------- Job Done for actors_movies ! ----------\n",
      "\u001b[0m\n",
      "2023-11-18 10:56:17 INFO     \u001b[38;5;2m---------- Start creating directors_movies ----------\u001b[0m\n",
      "2023-11-18 10:56:17 INFO     TSV files already exist.\n",
      "2023-11-18 10:56:17 INFO     File directors_test.parquet not found. Creation...\n",
      "2023-11-18 10:56:17 INFO     Parquet loaded ! Importing characters_test...\n",
      "2023-11-18 10:56:24 INFO     Get directors_test only...\n",
      "2023-11-18 10:56:27 INFO     Writing directors_test dataframe...\n",
      "2023-11-18 10:56:29 INFO     Dataframe directors_test ready to use!\n",
      "2023-11-18 10:56:29 INFO     Parquet loaded ! Importing persons_test...\n",
      "2023-11-18 10:56:30 INFO     Parquet loaded ! Importing movies_cleaned_test...\n",
      "2023-11-18 10:56:30 INFO     Values modified ? False\n",
      "2023-11-18 10:56:30 INFO     No need to update movies, all values are equals.\n",
      "2023-11-18 10:56:30 INFO     Dataframe movies_cleaned_test ready to use!\n",
      "2023-11-18 10:56:30 INFO     Parquet loaded ! Importing movies_cleaned_test...\n",
      "2023-11-18 10:56:33 INFO     Order and Rename...\n",
      "2023-11-18 10:56:33 INFO     Replace tt by movies titles...\n",
      "2023-11-18 10:56:33 INFO     Writing directors_movies_test dataframe...\n",
      "2023-11-18 10:56:34 INFO     Dataframe directors_movies_test ready to use!\n",
      "2023-11-18 10:56:34 INFO     \u001b[38;5;2m---------- Job Done for directors_movies ! ----------\n",
      "\u001b[0m\n",
      "2023-11-18 10:56:34 INFO     \u001b[38;5;2m-------------------- Job Done for 4 dataframes ! --------------------\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from get_dataframes_full_polars import GetDataframes\n",
    "# from get_dataframes import GetDataframes\n",
    "config = import_config()\n",
    "test = GetDataframes(config)\n",
    "\n",
    "# pl.read_parquet(\"clean_datasets/characters_test.parquet\")\n",
    "df = test.get_all_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directors = import_datasets_pl(\"clean_datasets/directors_test.parquet\", \"parquet\")\n",
    "persons = import_datasets_pl(\"clean_datasets/persons_test.parquet\", \"parquet\")\n",
    "movies = import_datasets_pl(\"clean_datasets/movies_test.parquet\", \"parquet\")\n",
    "\n",
    "directors_names = directors.join(persons, on=\"nconst\")\n",
    "\n",
    "movies_directors = movies.join(\n",
    "    directors_names,\n",
    "    left_on=\"titre_id\",\n",
    "    right_on=\"tconst\",\n",
    ")\n",
    "\n",
    "movies_directors = order_and_rename_pl(\n",
    "    movies_directors,\n",
    "    col_to_keep(\"directors_movies\"),\n",
    "    col_renaming(\"directors_movies\")\n",
    ")\n",
    "movies_directors = movies_directors.to_pandas()\n",
    "\n",
    "# movies_actors = movies_actors[col_renaming(name)]\n",
    "logging.info(\"Replace tt by movies titles...\")\n",
    "dict_titre = (\n",
    "    movies_actors[[\"titre_id\", \"titre_str\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"titre_id\")\n",
    "    .to_dict()[\"titre_str\"]\n",
    ")\n",
    "movies_actors[\"person_film\"] = movies_actors[\n",
    "    \"person_film\"\n",
    "].apply(\n",
    "    lambda x: if_tt_remove(\n",
    "        replace_ids_with_titles(x, dict_titre)\n",
    "    )\n",
    ")\n",
    "df = pl.from_pandas(movies_actors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import ast\n",
    "import pprint\n",
    "\n",
    "\n",
    "api_key = \"fe4a6f12753fa6c12b0fc0253b5e667f\"\n",
    "language = \"fr-FR\"\n",
    "imdb_id = 120\n",
    "\n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    'include_adult': \"False\",\n",
    "    \"language\": language,\n",
    "    # \"append_to_response\": \"keywords,credits,videos\"\n",
    "    # \"append_to_response\": \"keywords,credits,videos\"\n",
    "}\n",
    "\n",
    "find = False\n",
    "\n",
    "# if find:\n",
    "#     params.update(\n",
    "#         {\"external_source\" : \"imdb_id\"}\n",
    "#     )\n",
    "#     find_id = \"https://api.themoviedb.org/3/find/\"\n",
    "# else:\n",
    "base_url = \"https://api.themoviedb.org/3/movie/\"\n",
    "person_url = \"https://api.themoviedb.org/3/person/\"\n",
    "\n",
    "url = f\"{base_url}{imdb_id}\"\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "data = r.json()\n",
    "\n",
    "cc = [\n",
    "    # (\"keywords\", \"keywords\", \"name\", \"keywords\"),\n",
    "    (\"genres\", \"genres\", \"name\"),\n",
    "    (\"spoken_languages\", \"spoken_languages\", \"iso_639_1\"),\n",
    "    (\"production_companies_name\", \"production_companies\", \"name\"),\n",
    "    (\"production_countries\", \"production_countries\", \"iso_3166_1\"),\n",
    "    (\"production_companies_country\", \"production_companies\", \"origin_country\"),\n",
    "]\n",
    "\n",
    "for k, c, v, *sb in cc:\n",
    "    if sb:\n",
    "        data[k] = [k[v] for k in data[c][sb[0]]]\n",
    "    else:\n",
    "        data[k] = [k[v] for k in data[c]]\n",
    "\n",
    "# data[\"keywords\"] = [n[\"name\"] for n in data[\"keywords\"][\"keywords\"][:4]]\n",
    "# data[\"actors\"] = [n[\"name\"] for n in data[\"credits\"][\"cast\"] if n[\"known_for_department\"] == \"Acting\" and n[\"order\"] <= 2]\n",
    "# data[\"director\"] = [n[\"name\"] for n in data[\"credits\"][\"crew\"] if n[\"job\"] == \"Director\"]\n",
    "# data[\"url\"] = f\"https://www.imdb.com/title/{data['imdb_id']}\"\n",
    "# data[\"image\"] = f\"https://image.tmdb.org/t/p/w300_and_h450_bestv2{data['poster_path']}\"\n",
    "# data[\"youtube\"] = [f\"https://www.youtube.com/watch?v={n['key']}\" for n in data[\"videos\"][\"results\"]]\n",
    "\n",
    "to_pop = (\n",
    "    \"belongs_to_collection\",\n",
    "    \"production_companies\",\n",
    ")\n",
    "for tp in to_pop:\n",
    "    data.pop(tp)\n",
    "\n",
    "data[\"release_date\"] = data[\"release_date\"][:4]\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "df =pd.read_csv(\"movies_datasets/tmdb_full.csv\", low_memory=False)\n",
    "\n",
    "def decode_clean(item: str) -> str:\n",
    "    return item.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "\n",
    "def safe_literal_eval(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s) if isinstance(s, str) else s\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "col = [\n",
    "    \"genres\",\n",
    "    \"spoken_languages\",\n",
    "    \"production_companies_name\",\n",
    "    \"production_countries\",\n",
    "    \"production_companies_country\"\n",
    "]\n",
    "\n",
    "for c in col:\n",
    "    df[c] = df[c].apply(lambda x: [decode_clean(item) for item in safe_literal_eval(x) if isinstance(item, str) and item.strip()] if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.release_date = pd.to_datetime(df.release_date, format=\"%Y-%m-%d\")\n",
    "# df['release_date'] = df['release_date'].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "df.to_parquet(\"movies_datasets/tmdb.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"movies_datasets/tmdb.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import ast\n",
    "import pprint\n",
    "\n",
    "# if find:\n",
    "#     params.update(\n",
    "#         {\"external_source\" : \"imdb_id\"}\n",
    "#     )\n",
    "#     find_id = \"https://api.themoviedb.org/3/find/\"\n",
    "# else:\n",
    "\n",
    "\n",
    "api_key = \"fe4a6f12753fa6c12b0fc0253b5e667f\"\n",
    "language = \"fr-FR\"\n",
    "# imdb_id = 120\n",
    "imdb_id = 65731\n",
    "\n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    'include_adult': \"False\",\n",
    "    \"language\": language,\n",
    "    \"append_to_response\": \"combined_credits\"\n",
    "}\n",
    "\n",
    "find = False\n",
    "\n",
    "base_url = \"https://api.themoviedb.org/3/person/\"\n",
    "url_image = \"https://image.tmdb.org/t/p/w300_and_h450_bestv2\"\n",
    "url_youtube = \"https://www.youtube.com/watch?v=\"\n",
    "url = f\"{base_url}{imdb_id}\"\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "data = r.json()\n",
    "\n",
    "# cc = [\n",
    "#     # (\"keywords\", \"keywords\", \"name\", \"keywords\"),\n",
    "#     (\"genres\", \"genres\", \"name\"),\n",
    "#     (\"spoken_languages\", \"spoken_languages\", \"iso_639_1\"),\n",
    "#     (\"production_companies_name\", \"production_companies\", \"name\"),\n",
    "#     (\"production_countries\", \"production_countries\", \"iso_3166_1\"),\n",
    "# ]\n",
    "\n",
    "# for k, c, v, *sb in cc:\n",
    "#     if sb:\n",
    "#         data[k] = [k[v] for k in data[c][sb[0]]]\n",
    "#     else:\n",
    "#         data[k] = [k[v] for k in data[c]]\n",
    "import numpy as np\n",
    "data[\"image\"] = f\"{url_image}{data['profile_path']}\"\n",
    "# data[\"youtube\"] = [f\"{url_youtube}{n['key']}\" for n in data[\"videos\"][\"results\"]]\n",
    "total_movies = [n[\"adult\"] for n in data[\"combined_credits\"][\"cast\"]]\n",
    "# max_ = Counter(total_movies)\n",
    "# print(max_[0])\n",
    "moyenne_film = sorted([n[\"popularity\"] for n in data[\"combined_credits\"][\"cast\"][:5]], reverse=True)\n",
    "vote_count_film = sorted([n[\"vote_count\"] for n in data[\"combined_credits\"][\"cast\"][:5]], reverse=True)\n",
    "# mean = mean_(sorted(moyenne_film))\n",
    "quantile = np.quantile(sorted(moyenne_film), q=0.75)\n",
    "print(quantile)\n",
    "data[\"top_5\"]  =  sorted([n[\"original_title\"] for n in data[\"combined_credits\"][\"cast\"][:5] if n[\"popularity\"] >= quantile], reverse=True)\n",
    "data[\"top_5_images\"] =  [f\"{url_image}{n['poster_path']}\" for n in data[\"combined_credits\"][\"cast\"][:5] if n[\"popularity\"] >= quantile]\n",
    "# print(film_connu)\n",
    "# data[\"film_connu\"] = {\n",
    "# }\n",
    "\n",
    "to_pop = (\n",
    "    \"adult\",\n",
    "    \"also_known_as\",\n",
    "    \"gender\",\n",
    "    \"homepage\",\n",
    "    \"profile_path\",\n",
    "    'combined_credits',\n",
    "    \"known_for_department\",\n",
    "    \"imdb_id\",\n",
    "    \"popularity\",\n",
    ")\n",
    "for tp in to_pop:\n",
    "    data.pop(tp)\n",
    "\n",
    "# data[\"release_date\"] = data[\"release_date\"][:4]\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmd.dropna(subset=[\"release_date\"], inplace=True)\n",
    "tmd.reset_index(drop=\"index\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmd[\"release_date\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmd[\"release_date\"] = pd.to_datetime(tmd[\"release_date\"])\n",
    "tmd[\"release_date\"] = tmd[\"release_date\"].apply(lambda x:x.strftime(\"%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    tmd[\"\"]\n",
    ")\n",
    "tmd[tmd[\"adult\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import ast\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "api_key = \"fe4a6f12753fa6c12b0fc0253b5e667f\"\n",
    "language = \"en-US\"\n",
    "imdb_id = 19995\n",
    "\n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    'include_adult': \"False\",\n",
    "    \"language\": language,\n",
    "    \"append_to_response\": \"keywords,credits,videos\"\n",
    "}\n",
    "\n",
    "base_url = \"https://api.themoviedb.org/3/movie/\"\n",
    "\n",
    "url = f\"{base_url}{imdb_id}\"\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "data = r.json()\n",
    "\n",
    "cleaning = (\n",
    "    (\"keywords\", \"keywords\", \"name\", \"keywords\"),\n",
    "    (\"genres\",\"genres\", \"name\"),\n",
    "    (\"spoken_languages\", \"spoken_languages\", \"iso_639_1\"),\n",
    "    (\"production_companies_name\", \"production_companies\", \"name\"),\n",
    "    (\"production_countries\", \"production_countries\", \"iso_3166_1\"),\n",
    ")\n",
    "for key, category, value, *subkey in cleaning:\n",
    "    if subkey:\n",
    "        data[key] = [k[value] for k in data[category][subkey[0]]]\n",
    "    else:\n",
    "        data[key] = [k[value] for k in data[category]]\n",
    "data.pop(\"homepage\")\n",
    "data.pop(\"belongs_to_collection\")\n",
    "data.pop(\"production_companies\")\n",
    "pprint.pprint(data)\n",
    "\n",
    "df = pd.DataFrame([data])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning = (\n",
    "    (\"name\", \"keywords\", \"keywords\"),\n",
    "    (\"name\", \"genres\"),\n",
    "    (\"iso_639_1\", \"spoken_languages\"),\n",
    "    (\"name\", \"production_companies\"),\n",
    "    (\"iso_3166_1\", \"production_countries\"),\n",
    ")\n",
    "\n",
    "for key, category, *subkey in cleaning:\n",
    "    if subkey:\n",
    "        data[category] = [item[key] for item in data[category][subkey[0]]]\n",
    "    else:\n",
    "        data[category] = [item[key] for item in data[category]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fake_list(s):\n",
    "    t = []\n",
    "    for i in ast.literal_eval(s):\n",
    "        print(i)\n",
    "\n",
    "df[\"keywords\"] = df[\"keywords\"].apply(clean_fake_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmd = import_datasets(\"clean_datasets/tmdb_full.parquet\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from time import time\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "def date_to_ts(d):\n",
    "    return int(parser.parse(d).replace(tzinfo=datetime.timezone.utc).timestamp() * 1000)\n",
    "\n",
    "def utc_ms() -> float:\n",
    "    return datetime.utcnow().timestamp() * 1000\n",
    "\n",
    "\n",
    "now = str(datetime.now())\n",
    "\n",
    "def date_to_ts_now():\n",
    "    return int(datetime.timestamp(datetime.now()))\n",
    "now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = import_config()\n",
    "datas = GetDataframes(config)\n",
    "link = \"movies_cleaned\"\n",
    "df = datas.get_dataframes(\n",
    "    link,\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"titre_id\"].str.contains(\"tt5013056\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthese_docstrings = (\n",
    "\"Compréhension de la Méthode : Comprendre la méthode, ses paramètres, \" +\n",
    "\"processus internes, et sorties. \" +\n",
    "\"Utilisation du Format Numpy : Utiliser le format de docstring Numpy pour \" +\n",
    "\"sa clarté dans la description des paramètres et des valeurs de retour. \" +\n",
    "\"Structure du Docstring : Inclure : \" +\n",
    "\"- Brève Description : Résumer la fonction de la méthode en une ou deux lignes. \" +\n",
    "\"- Parameters : Lister et décrire tous les paramètres. \" +\n",
    "\"- Returns : Expliquer les valeurs retournées par la méthode. \" +\n",
    "\"Rédaction Précise et Limitée : Être concis et précis, respecter la limite \" +\n",
    "\"de 75 caractères (espace compris) par ligne selon PEP8. \" +\n",
    "\"Terminologie Technique Adéquate : Utiliser un langage technique approprié, \" +\n",
    "\"par exemple des termes spécifiques aux DataFrames pour les méthodes les concernant. \"\n",
    ")\n",
    "print(synthese_docstrings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (\"Je suis une IA spécialisée en Python scientifique, avec une expertise dans la rédaction de code et de documentation en français. Mon approche consiste à analyser et comprendre le code en profondeur avant de rédiger des docstrings. Ces dernières sont créées dans le style numpy, visant à être concises, claires, et faciles à comprendre. Je respecte scrupuleusement les règles du PEP8, notamment la limite de 65 caractères par ligne pour garantir une excellente lisibilité. Mon objectif est de fournir des explications détaillées non seulement sur le fonctionnement des fonctions, mais aussi sur leur rôle et leur intégration dans un script, assurant ainsi une compréhension complète pour les utilisateurs.\")\n",
    "t = t.split(\".\")\n",
    "t\n",
    "p = (\n",
    "    \"Je suis une IA spécialisée en Python scientifique, avec une expertise dans la rédaction de code et de documentation en français\" +\n",
    "    \"Mon approche consiste à analyser et comprendre le code en profondeur avant de rédiger des docstrings\" +\n",
    "    \"Ces dernières sont créées dans le style numpy en anglais c'est a dire (Methods, Atributs, Parameters, Returns, Raises, Notes etc...), visant à être concises, claires, et faciles à comprendre\" +\n",
    "    \"Je respecte scrupuleusement les règles du PEP8, notamment la limite de 65 caractères par ligne pour garantir une excellente lisibilité\" +\n",
    "    \"Mon objectif est de fournir des explications détaillées non seulement sur le fonctionnement des fonctions\" +\n",
    "    \"mais aussi sur leur rôle et leur intégration dans un script, assurant ainsi une compréhension complète pour les utilisateurs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_description = (\n",
    "    \"La documentation de la fonction doit débuter par une explication concise et \"\n",
    "    \"directe de son but principal, évitant l'usage d'un titre 'description'. Ensuite, \"\n",
    "    \"chaque paramètre est décrit individuellement, mentionnant son type, \"\n",
    "    \"son rôle et toute contrainte ou valeur par défaut pertinente. Si la fonction \"\n",
    "    \"renvoie quelque chose, la nature de cette valeur de retour est également \"\n",
    "    \"détaillée, en précisant son type et ce qu'elle représente dans le contexte de \"\n",
    "    \"la fonction. Des sections supplémentaires peuvent être ajoutées pour évoquer \"\n",
    "    \"des exceptions potentielles levées par la fonction et des notes supplémentaires, \"\n",
    "    \"si nécessaire. Cependant, il est important de garder ces sections courtes et \"\n",
    "    \"pertinentes. Tout au long de la documentation, le style d'écriture doit être clair, \"\n",
    "    \"concis et conforme aux normes PEP8, en veillant à ce que chaque ligne ne dépasse \"\n",
    "    \"pas 70 caractères. Cela facilite la lisibilité et la maintenance du code. Le formatage \"\n",
    "    \"Black, connu pour son style épuré et sa cohérence, doit être appliqué pour assurer \"\n",
    "    \"une présentation uniforme et professionnelle du document.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"machine_learning\"\n",
    "\n",
    "logging.info(f\"Creating {name} dataframe...\")\n",
    "tmdb_l = \"clean_datasets/tmdb_updated.parquet\"\n",
    "actors_l = \"clean_datasets/actors_movies.parquet\"\n",
    "directors_l = \"clean_datasets/directors_movies.parquet\"\n",
    "movies_l = \"clean_datasets/movies_cleaned.parquet\"\n",
    "\n",
    "tmdb = import_datasets(tmdb_l, \"parquet\")\n",
    "actors = import_datasets(actors_l, \"parquet\")\n",
    "directors = import_datasets(directors_l, \"parquet\")\n",
    "movies = import_datasets(movies_l, \"parquet\")\n",
    "\n",
    "col_to_keep = [\n",
    "    \"titre_id\",\n",
    "    \"titre_str\",\n",
    "    \"titre_genres\",\n",
    "    \"rating_avg\",\n",
    "    \"rating_votes\"\n",
    "]\n",
    "movies = movies[col_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directors_list_id = directors[\"titre_id\"]\n",
    "condi = movies[\"titre_id\"].isin(directors_list_id)\n",
    "condi2 = actors[\"titre_id\"].isin(directors_list_id)\n",
    "movies = movies[condi]\n",
    "actors = actors[condi2]\n",
    "\n",
    "actors_list_id = actors[\"titre_id\"]\n",
    "condi = movies[\"titre_id\"].isin(actors_list_id)\n",
    "condi2 = directors[\"titre_id\"].isin(actors_list_id)\n",
    "movies = movies[condi]\n",
    "directors = directors[condi2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(directors_list_id))\n",
    "print(len(actors_list_id))\n",
    "print(len(movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_to_keep = [\n",
    "    \"imdb_id\",\n",
    "    \"overview\"\n",
    "]\n",
    "tmdb = tmdb[col_to_keep]\n",
    "\n",
    "col_to_keep = [\n",
    "    \"titre_id\",\n",
    "    \"person_name\",\n",
    "    # \"person_index\"\n",
    "]\n",
    "actors = actors[col_to_keep]\n",
    "\n",
    "col_to_keep = [\n",
    "    \"titre_id\",\n",
    "    \"person_name\",\n",
    "    # \"person_index\"\n",
    "]\n",
    "directors = directors[col_to_keep]\n",
    "\n",
    "actors.loc[:, \"person_name\"] = actors[\"person_name\"].str.split(\", \")\n",
    "directors.loc[:, \"person_name\"] = directors[\"person_name\"].str.split(\", \")\n",
    "\n",
    "person_name = actors.groupby(\"titre_id\")[\"person_name\"].sum().reset_index()\n",
    "person_list = person_name[\"person_name\"].to_list()\n",
    "\n",
    "directors_name = directors.groupby(\"titre_id\")[\"person_name\"].sum().reset_index()\n",
    "directors_list = directors_name[\"person_name\"].to_list()\n",
    "\n",
    "movies[\"actors\"] = person_list\n",
    "movies[\"directors\"] = directors_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(directors_list_id))\n",
    "print(len(actors_list_id))\n",
    "print(len(movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.info(f\"Merging {name} dataframe...\")\n",
    "ml_df = pd.merge(\n",
    "    movies,\n",
    "    tmdb,\n",
    "    left_on = \"titre_id\",\n",
    "    right_on = \"imdb_id\"\n",
    ")\n",
    "\n",
    "logging.info(f\"Droping NaN {name} dataframe...\")\n",
    "ml_df.drop([\"imdb_id\"], axis = 1, inplace = True)\n",
    "ml_df[ml_df.isna().any(axis=1)]\n",
    "ml_df.dropna(inplace=True)\n",
    "\n",
    "# tt = (\n",
    "#     (\"actors\", \"actors\"),\n",
    "#     (\"titre_genres\", \"titre_genres\"),\n",
    "#     (\"directors\", \"directors\"),\n",
    "# )\n",
    "# for t in tt:\n",
    "#     ml_df[t[0]] = ml_df[t[1]].apply(\n",
    "#         lambda x: \", \".join(map(str, x))\n",
    "#     ).replace(\" \", \"\")\n",
    "\n",
    "# # Full loWer pour reduire les titres, actors, directors etc...\n",
    "# # for t in tt:\n",
    "# #     ml_df[t[0]] = ml_df[t[1]].apply(full_lower)\n",
    "# logging.info(f\"Process Overview...\")\n",
    "# ml_df['overview'] = ml_df['overview'].astype(str).apply(clean_overview)\n",
    "\n",
    "# logging.info(f\"Writing {name} dataframe...\")\n",
    "# ml_df.to_parquet(path_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hjson\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import process\n",
    "from unicodedata import normalize, combining\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from get_dataframes import GetDataframes\n",
    "from tools import import_config, import_datasets, check_titre, color\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = import_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 14:15:09 INFO     Parquet loaded ! Importing machine_learning_final...\n"
     ]
    }
   ],
   "source": [
    "name = \"clean_datasets/machine_learning_final.parquet\"\n",
    "df = import_datasets(name, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titre_index(titre: str):\n",
    "    return df[df.titre_str == titre].index[0]\n",
    "\n",
    "\n",
    "def director_index(director: str):\n",
    "    return df[df.directors.str.contains(director)].index[0]\n",
    "\n",
    "\n",
    "def actor_index(actor: str):\n",
    "    return df[df.actors.str.contains(actor)].index[0]\n",
    "\n",
    "\n",
    "def idx_titre(idx: int):\n",
    "    return df[df.index == idx][\"titre_str\"].values[0]\n",
    "\n",
    "\n",
    "def idx_actor(idx: int):\n",
    "    return df[df.index == idx][\"actors\"].values[0]\n",
    "\n",
    "\n",
    "def idx_titre_id(idx: int):\n",
    "    return df[df.index == idx][\"titre_id\"].values[0]\n",
    "\n",
    "\n",
    "def idx_popularity(idx: int):\n",
    "    return df[df.index == idx][\"popularity\"].values[0]\n",
    "\n",
    "\n",
    "def idx_keywords(idx: int):\n",
    "    return df[df.index == idx][\"keywords\"].values[0]\n",
    "\n",
    "\n",
    "def idx_image(idx: int):\n",
    "    return df[df.index == idx][\"image\"].values[0]\n",
    "\n",
    "\n",
    "def idx_youtube(idx: int):\n",
    "    return df[df.index == idx][\"youtube\"].values[0]\n",
    "\n",
    "\n",
    "def idx_url(idx: int):\n",
    "    return df[df.index == idx][\"url\"].values[0]\n",
    "\n",
    "\n",
    "def check_titre_str(d: pd.DataFrame, movie: str):\n",
    "    return df[df[\"titre_str\"].str.contains(movie)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'titre_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\morga\\OneDrive\\AppData\\Bureau\\WildCodeSchool\\z_project_2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\morga\\OneDrive\\AppData\\Bureau\\WildCodeSchool\\z_project_2\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\morga\\OneDrive\\AppData\\Bureau\\WildCodeSchool\\z_project_2\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'titre_clean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\morga\\OneDrive\\AppData\\Bureau\\WildCodeSchool\\z_project_2\\ML.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/morga/OneDrive/AppData/Bureau/WildCodeSchool/z_project_2/ML.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m check_titre(df, \u001b[39m\"\u001b[39;49m\u001b[39mring\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\morga\\OneDrive\\AppData\\Bureau\\WildCodeSchool\\z_project_2\\tools.py:752\u001b[0m, in \u001b[0;36mcheck_titre\u001b[1;34m(df, string, max)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     string \u001b[39m=\u001b[39m (\n\u001b[0;32m    746\u001b[0m         string\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    747\u001b[0m         \u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[39m.\u001b[39mlower()\n\u001b[0;32m    751\u001b[0m     )\n\u001b[1;32m--> 752\u001b[0m     \u001b[39mreturn\u001b[39;00m df[df[\u001b[39m\"\u001b[39;49m\u001b[39mtitre_clean\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(string)][:\u001b[39mmax\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\morga\\OneDrive\\AppData\\Bureau\\WildCodeSchool\\z_project_2\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\morga\\OneDrive\\AppData\\Bureau\\WildCodeSchool\\z_project_2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'titre_clean'"
     ]
    }
   ],
   "source": [
    "check_titre(df, \"ring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(r):\n",
    "    return (\n",
    "        r[\"keywords\"]\n",
    "        + \" \"\n",
    "        + r[\"actors\"]\n",
    "        + \" \"\n",
    "        + r[\"director\"]\n",
    "        + \" \"\n",
    "        +\n",
    "        # r[\"overview\"]+\" \"+\n",
    "        r[\"titre_genres\"]\n",
    "        # str(r[\"date_only\"])\n",
    "        # str(r[\"popularity\"])\n",
    "        # str(r[\"rating_avg\"])+ \" \"+\n",
    "        # str(r[\"rating_vote\"])\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"one_for_all\"] = df.apply(combine, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_index_tfidf(movies: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Utilisation de FuzzyWuzzy et TfidfVectorizer\n",
    "    \"\"\"\n",
    "    best_match = process.extract(\n",
    "        movies, df[\"titre_clean\"].values, limit=10\n",
    "    )\n",
    "    best_candidate = [match[0] for match in best_match]\n",
    "    print(\"best_matches\", best_match)\n",
    "    print(\"best_candidate\", best_candidate)\n",
    "    print(\"movies\", movies)\n",
    "\n",
    "    small_df = df[df[\"titre_clean\"].isin(best_candidate)]\n",
    "    print(\n",
    "        small_df[\n",
    "            [\"titre_id\", \"titre_str\", \"titre_genres\", \"keywords\"]\n",
    "        ].to_markdown()\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    matrix = tfidf.fit_transform(best_candidate)\n",
    "    tfidf_ = tfidf.transform([movies])\n",
    "\n",
    "    cosine_similarities = cosine_similarity(tfidf_, matrix).flatten()\n",
    "\n",
    "    best_match_idx = cosine_similarities.argmax()\n",
    "    best_match_titre = best_candidate[best_match_idx]\n",
    "    print(\"best_match_idx\", best_match_idx)\n",
    "    print(\"best_match_titre\", best_match_titre)\n",
    "    return df[df[\"titre_clean\"] == best_match_titre].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_index_knn(movies: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Utilisation de FuzzyWuzzy et Nearest Neighbors\n",
    "    \"\"\"\n",
    "    # best_match = process.extract(movies, df['titre_clean'].values, limit=10)\n",
    "    best_match = process.extractOne(movies, df[\"titre_clean\"].values)\n",
    "    # best_candidate = [match[0] for match in best_match]\n",
    "    # print(\"best_matches :\\n\",best_match[0])\n",
    "    # print(\"best_candidate :\\n\",best_candidate)\n",
    "    small_df = df[df[\"titre_clean\"] == best_match[0]]\n",
    "    # print(small_df[[\"titre_id\", \"titre_str\", \"titre_genres\", \"keywords\"]].to_markdown())\n",
    "    # small_df = df[df['titre_clean'].isin(best_candidate)]\n",
    "    return df[df[\"titre_clean\"] == best_match[0]].index[0]\n",
    "    print(\n",
    "        small_df[\n",
    "            [\"titre_id\", \"titre_str\", \"titre_genres\", \"keywords\"]\n",
    "        ].to_markdown()\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    matrix = tfidf.fit_transform(small_df[\"titre_clean\"].values)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=1).fit(matrix)\n",
    "    vector = tfidf.transform([movies])\n",
    "\n",
    "    print(\"query_vector\\n\", vector)\n",
    "\n",
    "    dist, idx = knn.kneighbors(vector, return_distance=True)\n",
    "\n",
    "    best_match_idx = idx[0][0]\n",
    "    best_match_titre = small_df[\"titre_clean\"].iloc[best_match_idx]\n",
    "    print(\"best_match_idx :\", best_match_idx)\n",
    "    print(\"best_match_titre :\", best_match_titre)\n",
    "    print()\n",
    "    return small_df[small_df[\"titre_clean\"] == best_match_titre].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_index_rf(movies: str, df: pd.DataFrame):\n",
    "    # Je capte pas le fonctionnement, j'ai besoin de plus de recherche\n",
    "    raise NotImplementedError\n",
    "    \"\"\"\n",
    "    Utilisation de FuzzyWuzzy et RandomForestClassifier\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X = tfidf.fit_transform(df[\"titre_clean\"].values)\n",
    "    y = df[\"titre_clean\"].values\n",
    "\n",
    "    y_encoded = LabelEncoder().fit_transform(y)\n",
    "    rf = RandomForestClassifier().fit(X, y_encoded)\n",
    "\n",
    "    vector = tfidf.transform([movies])\n",
    "    prediction = rf.predict(vector)\n",
    "\n",
    "    predict = y_encoded.inverse_transform(prediction)[0]\n",
    "    return df[df[\"titre_clean\"] == predict].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_algo(movies: str, df: pd.DataFrame, algo: str = \"tfidf\"):\n",
    "    movies = (\n",
    "        movies.replace(\" \", \"\")\n",
    "        .replace(\"-\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\":\", \"\")\n",
    "        .lower()\n",
    "    )\n",
    "    if algo == \"tfidf\":\n",
    "        return get_best_match_index_tfidf(movies, df)\n",
    "    elif algo == \"knn\":\n",
    "        return get_best_match_index_knn(movies, df)\n",
    "    elif algo == \"rf\":\n",
    "        return get_best_match_index_rf(movies, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_algo(\n",
    "    df: pd.DataFrame, movies: str, top: int = 10, algo: str = \"tfidf\"\n",
    "):\n",
    "    poids_ = {\n",
    "        \"titre_genres\": 0.2,\n",
    "        \"actors\": 0.15,\n",
    "        \"director\": 0.15,\n",
    "        \"overview\": 0.2,\n",
    "        \"keywords\": 0.3,\n",
    "    }\n",
    "\n",
    "    full_matrix = []\n",
    "    for col, poids in poids_.items():\n",
    "        tfidf_ = TfidfVectorizer()\n",
    "        matrix_ = tfidf_.fit_transform(df[col]) * poids\n",
    "        full_matrix.append(matrix_)\n",
    "\n",
    "    combined_matrix = hstack(full_matrix)\n",
    "    cosine = cosine_similarity(combined_matrix)\n",
    "    mov_idx = pick_algo(movies, df, algo)\n",
    "\n",
    "    similar = cosine[mov_idx]\n",
    "    similar1 = list(enumerate(cosine[mov_idx]))\n",
    "\n",
    "    sim_scores = sorted(similar1, key=lambda x: x[1], reverse=True)\n",
    "    sim_mov_idx = similar.argsort()[::-1][1 : top + 1]\n",
    "\n",
    "    same_movies = df.loc[sim_mov_idx, \"titre_str\"]\n",
    "\n",
    "    sim_scores[1 : top + 1]\n",
    "    score = [i[1] for i in sim_scores]\n",
    "\n",
    "    # imdb_url = \"https://www.imdb.com/title/\"\n",
    "    # tmdb_image = \"https://image.tmdb.org/t/p/w500\"\n",
    "    poster = f\"Poster : {idx_image(mov_idx)}\\n\"\n",
    "\n",
    "    print(color(\"~\" * len(poster), \"red\"))\n",
    "    print(f\"Top 10 similar movies to {idx_titre(mov_idx)} are :\")\n",
    "    print(f\"Popularity {idx_popularity(mov_idx)}\")\n",
    "    print(f\"IMdb link : {idx_url(mov_idx)}\")\n",
    "    print(f\"Poster : {idx_image(mov_idx)}\")\n",
    "    print(f\"Youtube : {idx_youtube(mov_idx)}\")\n",
    "    print(color(\"~\" * len(poster), \"red\"))\n",
    "    print()\n",
    "    for movies_, idx in zip(same_movies, sim_mov_idx):\n",
    "        cmt = (\n",
    "            f\"Movie : {idx_titre(idx)} | popularity {idx_popularity(idx)}\\n\"\n",
    "            + f\"IMdb link : {idx_url(idx)}\\n\"\n",
    "            + f\"Poster : {idx_image(idx)}\\n\"\n",
    "            + f\"Youtube : {idx_youtube(idx)}\\n\"\n",
    "        )\n",
    "        line = cmt.split(\"\\n\")\n",
    "        print(cmt + color(\"-\" * len(max(line, key=len)), \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_algo(\n",
    "    df: pd.DataFrame, movies: str, top: int = 5, algo: str = \"knn\"\n",
    "):\n",
    "    cv = CountVectorizer()\n",
    "    count_matrix = cv.fit_transform(df[\"one_for_all\"])\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "    mov_idx = pick_algo(movies, df, algo)\n",
    "\n",
    "    sim_scores = list(enumerate(cosine_sim[mov_idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1 : top + 1]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # imdb_url = \"https://www.imdb.com/title/\"\n",
    "    # tmdb_image = \"https://image.tmdb.org/t/p/w500\"\n",
    "    poster = f\"Poster : {idx_image(mov_idx)}\\n\"\n",
    "\n",
    "    print(color(\"~\" * len(poster), \"red\"))\n",
    "    print(f\"Top 10 similar movies to {idx_titre(mov_idx)} are :\")\n",
    "    print(f\"Popularity {idx_popularity(mov_idx)}\")\n",
    "    print(f\"IMdb link : {idx_url(mov_idx)}\")\n",
    "    print(f\"Poster : {idx_image(mov_idx)}\")\n",
    "    print(f\"Youtube : {idx_youtube(mov_idx)}\")\n",
    "    print(color(\"~\" * len(poster), \"red\"))\n",
    "    print()\n",
    "    for idx in movie_indices:\n",
    "        cmt = (\n",
    "            f\"Movie : {idx_titre(idx)} | popularity {idx_popularity(idx)}\\n\"\n",
    "            + f\"IMdb link : {idx_url(idx)}\\n\"\n",
    "            + f\"Poster : {idx_image(idx)}\\n\"\n",
    "            + f\"Youtube : {idx_youtube(idx)}\\n\"\n",
    "        )\n",
    "        line = cmt.split(\"\\n\")\n",
    "        print(cmt + color(\"-\" * len(max(line, key=len)), \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_algo(\n",
    "    df: pd.DataFrame, movies: str, top: int = 5, algo: str = \"knn\"\n",
    "):\n",
    "    cv = CountVectorizer()\n",
    "    count_matrix = cv.fit_transform(df[\"one_for_all\"])\n",
    "    mov_idx = pick_algo(movies, df, algo)\n",
    "\n",
    "    knn_model = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\").fit(\n",
    "        count_matrix\n",
    "    )\n",
    "    dist, indices = knn_model.kneighbors(\n",
    "        count_matrix[mov_idx], n_neighbors=top + 1\n",
    "    )\n",
    "\n",
    "    poster = f\"Poster : {idx_image(mov_idx)}\\n\"\n",
    "    print(color(\"~\" * len(poster), \"red\"))\n",
    "    print(f\"Top 10 similar movies to {idx_titre(mov_idx)} are :\")\n",
    "    print(f\"Popularity {idx_popularity(mov_idx)}\")\n",
    "    print(f\"IMdb link : {idx_url(mov_idx)}\")\n",
    "    print(f\"Poster : {idx_image(mov_idx)}\")\n",
    "    print(f\"Youtube : {idx_youtube(mov_idx)}\")\n",
    "    print(color(\"~\" * len(poster), \"red\"))\n",
    "    print()\n",
    "    for idx, dis in zip(indices.flatten()[1:], dist.flatten()[1:]):\n",
    "        cmt = (\n",
    "            f\"Movie : {idx_titre(idx)} | popularity {idx_popularity(idx)}\\n\"\n",
    "            + f\"IMdb link : {idx_url(idx)}\\n\"\n",
    "            + f\"Poster : {idx_image(idx)}\\n\"\n",
    "            + f\"Youtube : {idx_youtube(idx)}\\n\"\n",
    "        )\n",
    "        line = cmt.split(\"\\n\")\n",
    "        print(cmt + color(\"-\" * len(max(line, key=len)), \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTOPLAY : \n",
    "https://www.youtube.com/embed/MJ3Up7By5cw?autoplay=1&autohide=2&border=0&wmode=opaque&enablejsapi=1&modestbranding=1&controls=0&showinfo=1&mute=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = \"fight club\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_algo(df, movies, algo=\"knn\", top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_algo(df, movies, algo=\"knn\", top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_algo(df, movies, algo=\"knn\", top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_forest_algo(df, movies, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

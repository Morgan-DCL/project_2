{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hjson\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import process\n",
    "from unicodedata import normalize, combining\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from get_dataframes import GetDataframes\n",
    "from tools import import_config, import_datasets, check_titre\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmd = import_datasets(\"clean_datasets/tmdb_updated.parquet\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = import_config()\n",
    "datas = GetDataframes(config)\n",
    "df = datas.get_dataframes(\n",
    "    \"machine_learning\",\n",
    "    cleaned=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"release_date\"] = df[\"release_date\"].apply(lambda x : x.strftime(\"%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmd = import_datasets(\"clean_datasets/tmdb_updated.parquet\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"titre_clean\"] = df[\"titre_str\"]\n",
    "df[\"titre_clean\"] = df[\"titre_clean\"].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_overview(\n",
    "    text: str\n",
    ") -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimer_accents(texte):\n",
    "    texte_clean = normalize('NFKD', texte)\n",
    "    return \"\".join(\n",
    "        [c for c in texte_clean if not combining(c)]\n",
    "    )\n",
    "\n",
    "tt = [\n",
    "    \"actors\",\n",
    "    \"titre_genres\",\n",
    "    \"directors\",\n",
    "    \"titre_clean\",\n",
    "]\n",
    "for t in tt:\n",
    "    df[t] = df[t].astype(str).apply(supprimer_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cleaning overview\")\n",
    "df['overview'] = df['overview'].astype(str).apply(clean_overview)\n",
    "# print(\"cleaning titre_clean\")\n",
    "# df['titre_clean'] = df['titre_clean'].astype(str).apply(clean_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_lower(text: str):\n",
    "    return text.replace(\" \", \"\").replace(\"-\", \"\").lower()\n",
    "\n",
    "tt = [\n",
    "    \"actors\",\n",
    "    \"titre_genres\",\n",
    "    \"directors\",\n",
    "    \"titre_clean\",\n",
    "]\n",
    "for t in tt:\n",
    "    print(f\"lowering everything in {t}\")\n",
    "    if \"titre_clean\" in t:\n",
    "        df[t] = df[t].apply(lambda x : x.replace(\":\", \" \"))\n",
    "        tt.remove(t)\n",
    "    else:\n",
    "        df[t] = df[t].apply(full_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff = df.copy()\n",
    "dfff.reset_index(drop='index', inplace=True)\n",
    "name = \"machine_learning.csv\"\n",
    "dfff.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"machine_learning.csv\"\n",
    "dff = pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titre_index(titre: str):\n",
    "    return dff[dff.titre_str == titre].index[0]\n",
    "\n",
    "def director_index(director: str):\n",
    "    return dff[dff.directors.str.contains(director)].index[0]\n",
    "\n",
    "def actor_index(actor: str):\n",
    "    return dff[dff.actors.str.contains(actor)].index[0]\n",
    "\n",
    "def idx_titre(idx: int):\n",
    "    return dff[dff.index == idx][\"titre_str\"].values[0]\n",
    "\n",
    "def idx_actor(idx: int):\n",
    "    return dff[dff.index == idx][\"actors\"].values[0]\n",
    "\n",
    "def idx_titre_id(idx: int):\n",
    "    return dff[dff.index == idx][\"titre_id\"].values[0]\n",
    "\n",
    "def idx_poster_path(idx: int):\n",
    "    return dff[dff.index == idx][\"poster_path\"].values[0]\n",
    "\n",
    "def idx_popularity(idx: int):\n",
    "    return dff[dff.index == idx][\"popularity\"].values[0]\n",
    "\n",
    "def check_titre_str(d: pd.DataFrame, movie: str):\n",
    "    return dff[dff[\"titre_str\"].str.contains(movie)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_titre(dff, \"oppenheimer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(r):\n",
    "    return (\n",
    "        r[\"titre_genres\"]+\" \"+\n",
    "        r[\"directors\"]+\" \"+\n",
    "        r[\"overview\"]+\" \"+\n",
    "        r[\"actors\"]+\" \"+\n",
    "        str(r[\"release_date\"])+\" \"+\n",
    "        str(r[\"popularity\"])+\" \"+\n",
    "        str(r[\"revenue\"])\n",
    "        # str(r[\"rating_avg\"])+ \" \"+\n",
    "        # str(r[\"rating_votes\"])\n",
    "    )\n",
    "\n",
    "dff[\"one_for_all\"] = dff.apply(combine, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_index_tfidf(movies: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Utilisation de FuzzyWuzzy et TfidfVectorizer\n",
    "    \"\"\"\n",
    "    # movies = movies.lower().replace(\" \", \"\")\n",
    "    best_match = process.extract(movies, df['titre_clean'].values, limit=10)\n",
    "    best_candidate = [match[0] for match in best_match]\n",
    "    print(\"best_matches\",best_match)\n",
    "    print(\"best_candidate\",best_candidate)\n",
    "    print(\"movies\",movies)\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    matrix = tfidf.fit_transform(best_candidate)\n",
    "    tfidf_ = tfidf.transform([movies])\n",
    "\n",
    "    cosine_similarities = cosine_similarity(tfidf_, matrix).flatten()\n",
    "\n",
    "    best_match_idx = cosine_similarities.argmax()\n",
    "    best_match_titre = best_candidate[best_match_idx]\n",
    "    print(\"best_match_idx\",best_match_idx)\n",
    "    print(\"best_match_titre\",best_match_titre)\n",
    "    return df[df['titre_clean'] == best_match_titre].index[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_index_knn(movies: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Utilisation de FuzzyWuzzy et Nearest Neighbors\n",
    "    \"\"\"\n",
    "    best_match = process.extract(movies, df['titre_clean'].values, limit=10)\n",
    "    best_candidate = [match[0] for match in best_match]\n",
    "    print(\"best_matches :\\n\",best_match)\n",
    "    print(\"best_candidate :\\n\",best_candidate)\n",
    "    print()\n",
    "\n",
    "    small_df = df[df['titre_clean'].isin(best_candidate)]\n",
    "    print(small_df[[\"titre_id\", \"titre_str\", \"titre_genres\"]].to_markdown())\n",
    "    print()\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    matrix = tfidf.fit_transform(small_df['titre_clean'].values)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=1).fit(matrix)\n",
    "    vector = tfidf.transform([movies])\n",
    "\n",
    "    print(\"query_vector\\n\",vector)\n",
    "\n",
    "    dist, idx = knn.kneighbors(vector, return_distance=True)\n",
    "\n",
    "    best_match_idx = idx[0][0]\n",
    "    best_match_titre = small_df['titre_clean'].iloc[best_match_idx]\n",
    "    print(\"best_match_idx :\",best_match_idx)\n",
    "    print(\"best_match_titre :\",best_match_titre)\n",
    "    print()\n",
    "    return small_df[small_df['titre_clean'] == best_match_titre].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_index_rf(movies: str, df: pd.DataFrame):\n",
    "    # Je capte pas le fonctionnement, j'ai besoin de plus de recherche\n",
    "    raise NotImplementedError\n",
    "    \"\"\"\n",
    "    Utilisation de FuzzyWuzzy et RandomForestClassifier\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X = tfidf.fit_transform(df['titre_clean'].values)\n",
    "    y = df['titre_clean'].values\n",
    "\n",
    "    y_encoded = LabelEncoder().fit_transform(y)\n",
    "    rf = RandomForestClassifier().fit(X, y_encoded)\n",
    "\n",
    "    vector = tfidf.transform([movies])\n",
    "    prediction = rf.predict(vector)\n",
    "\n",
    "    predict = y_encoded.inverse_transform(prediction)[0]\n",
    "    return df[df['titre_clean'] == predict].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_algo(movies: str, df: pd.DataFrame, algo: str = \"tfidf\"):\n",
    "    if algo == \"tfidf\":\n",
    "        return get_best_match_index_tfidf(movies, df)\n",
    "    elif algo == \"knn\":\n",
    "        return get_best_match_index_knn(movies, df)\n",
    "    elif algo == \"rf\":\n",
    "        return get_best_match_index_rf(movies, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_algo(df:pd.DataFrame, movies: str, top: int = 10, algo: str = \"tfidf\"):\n",
    "    poids_ = {\n",
    "        \"titre_genres\": 2,\n",
    "        \"actors\":       1.5,\n",
    "        \"directors\":    1.5,\n",
    "        \"overview\":     2.5,\n",
    "    }\n",
    "\n",
    "    full_matrix = []\n",
    "    for col, poids in poids_.items():\n",
    "        tfidf_ = TfidfVectorizer()\n",
    "        matrix_ = tfidf_.fit_transform(df[col]) * poids\n",
    "        full_matrix.append(matrix_)\n",
    "\n",
    "    combined_matrix = hstack(full_matrix)\n",
    "    cosine = cosine_similarity(combined_matrix)\n",
    "\n",
    "    mov_idx = pick_algo(movies, df, algo)\n",
    "    best_match = idx_titre(mov_idx)\n",
    "    mov_id = idx_titre_id(mov_idx)\n",
    "\n",
    "    similar = cosine[mov_idx]\n",
    "    similar1 = list(enumerate(cosine[mov_idx]))\n",
    "\n",
    "    sim_scores = sorted(similar1, key=lambda x: x[1], reverse=True)\n",
    "    sim_mov_idx = similar.argsort()[::-1][1:top+1]\n",
    "\n",
    "    same_movies = df.loc[sim_mov_idx, \"titre_str\"]\n",
    "    ttconst = df.loc[sim_mov_idx, \"titre_id\"]\n",
    "\n",
    "    sim_scores[1:top+1]\n",
    "    score = [i[1] for i in sim_scores]\n",
    "    print()\n",
    "    print(f\"Top 10 similar movies to {best_match} idx {mov_id} are :\\n\")\n",
    "    for movies_, tt,  score in zip(same_movies, ttconst, score):\n",
    "        print(f\"Movie : {movies_} | id : {tt} | score : {np.round(score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_algo(df: pd.DataFrame, movies: str, top: int = 5, algo: str = \"knn\"):\n",
    "    cv = CountVectorizer()\n",
    "    count_matrix = cv.fit_transform(df['one_for_all'])\n",
    "    mov_idx = pick_algo(movies, df, algo)\n",
    "    print(\"movie_index =\", mov_idx)\n",
    "\n",
    "    knn_model = NearestNeighbors(metric='cosine', algorithm='brute').fit(count_matrix)\n",
    "    dist, indices = knn_model.kneighbors(count_matrix[mov_idx], n_neighbors=top+1)\n",
    "    print()\n",
    "    print(f\"Top 10 similar movies to {idx_titre(mov_idx)} are :\")\n",
    "    print(f\"Popularity {idx_popularity(mov_idx)}\")\n",
    "    print(f\"IMdb link : https://www.imdb.com/title/{idx_titre_id(mov_idx)}\")\n",
    "    poster = f\"Poster : https://image.tmdb.org/t/p/w500{idx_poster_path(mov_idx)}\\n\"\n",
    "    print(poster+\"*\"*len(poster)+\"\\n\\n\")\n",
    "    for index, dis in zip(indices.flatten()[1:], dist.flatten()[1:]):\n",
    "        cmt = (\n",
    "            f\"Movie : {idx_titre(index)} | popularity {idx_popularity(index)} | score : {np.round(dis, 4)}\\n\" +\n",
    "            f\"IMdb link : https://www.imdb.com/title/{idx_titre_id(index)}\\n\"\n",
    "            f\"Poster : https://image.tmdb.org/t/p/w500{idx_poster_path(index)}\\n\")\n",
    "        line = cmt.split('\\n')\n",
    "        print(cmt+\"-\"*len(line[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies = \"platform\"\n",
    "# tfidf_algo(dff, movies, algo=\"knn\", top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = \"platform\"\n",
    "knn_algo(dff, movies, algo=\"knn\", top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_algo(df: pd.DataFrame, movies: str, top: int = 5, algo = \"knn\"):\n",
    "    mov_idx = pick_algo(movies, df, algo)\n",
    "    print(\"movie_index =\", mov_idx)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    count_matrix = cv.fit_transform(df['one_for_all'])\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['titre_genres'])\n",
    "\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(count_matrix, y)\n",
    "\n",
    "    predicted_genre = rf_model.predict(count_matrix[mov_idx])\n",
    "\n",
    "    same_genre_idx = np.where(y == predicted_genre[0])[0]\n",
    "\n",
    "    recommended_indices = np.random.choice(same_genre_idx, size=top, replace=False)\n",
    "\n",
    "    print()\n",
    "    print(f\"Top 10 similar movies to {idx_titre(mov_idx)} are :\")\n",
    "    print(f\"Popularity {idx_popularity(mov_idx)}\")\n",
    "    print(f\"IMdb link : https://www.imdb.com/title/{idx_titre_id(mov_idx)}\")\n",
    "    poster = f\"Poster : https://image.tmdb.org/t/p/w500{idx_poster_path(mov_idx)}\\n\"\n",
    "    print(poster+\"*\"*len(poster)+\"\\n\\n\")\n",
    "    for mov_id in recommended_indices:\n",
    "        cmt = (\n",
    "            f\"Movie : {idx_titre(mov_id)} | popularity {idx_popularity(mov_id)}\" +\n",
    "            f\"IMdb link : https://www.imdb.com/title/{idx_titre_id(mov_id)}\\n\"\n",
    "            f\"Poster : https://image.tmdb.org/t/p/w500{idx_poster_path(mov_id)}\\n\")\n",
    "        line = cmt.split('\\n')\n",
    "        print(cmt+\"-\"*len(line[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies = \"platform\"\n",
    "# random_forest_algo(dff, movies, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
